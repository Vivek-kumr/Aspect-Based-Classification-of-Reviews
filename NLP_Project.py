# -*- coding: utf-8 -*-
"""Text_Preprocessing_Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u_WBLl8TTKDuhP3uLWwQ3On987mapU0X
"""

#from google.colab import drive
#drive.mount('/content/drive',force_remount=False)

import nltk
import numpy as np
import json
import pandas as pd
#import matplotlib.pyplot as plt
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
from nltk.tokenize import TweetTokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import pandas as pd
import string
import gensim 
from gensim.models import KeyedVectors
import os
from gensim.models import Word2Vec

## Tokenized Output
## object creation
# stop_words = set(stopwords.words('english'))
# file = open('/content/drive/My Drive/amazon_review.txt')

def preprocess(s):
### Data Reading
  # z = file.readlines()
  # s = ""
  # s = s.join(z)


  ### Splitting input lines
  ### Tokenizing line by line
  stop_words = set(stopwords.words('english'))
  l = WordNetLemmatizer()
  t = TweetTokenizer() 
  for c in string.punctuation:   ### Removing Punctuations
      if c == string.punctuation[13]:
        s = s.replace(c," ")
      if c != string.punctuation[6] and c != string.punctuation[12]:
        s= s.replace(c,"")

  res = s.splitlines() 
  text = [] 
  for i in res:
    p = t.tokenize(i) ### Tokenization
    text.append(p)
    text.append('\n')

  ## Flatting the list

  flat_list = []
  for sublist in text:
      for item in sublist:
          flat_list.append(item)

  tokens = [token.lower() for token in flat_list]
 


  ### Combining list elements to string
  ### Tokenized output

  ### Lemmization
  lem_text =[]
  for c in tokens:
    tokenize_text = l.lemmatize(c,pos='v')
    tokenize_text = l.lemmatize(tokenize_text,pos='n')
    tokenize_text = l.lemmatize(tokenize_text,pos='a')
    lem_text.append(tokenize_text)

  ### Stopwords Removal
  stop_text = []
  for w in lem_text: 
      if w not in stop_words: 
          stop_text.append(w)
  # print("Processed Tokens",stop_text)
  tagged = nltk.pos_tag(stop_text) ### POS Tagging 
  return res, tagged

  # s = " "
  # output = s.join(stop_text)
  # print(output)

### Load gensim google news pre trained model

file_path = '/home/tushar.abhishek/GoogleNews-vectors-negative300.bin'
model = KeyedVectors.load_word2vec_format(file_path, binary=True)
word_vectors = model.wv

### word embedding 

def word_embedding(data):
  x = model.similarity(data,'good')
  y = model.similarity(data,'bad')
  z = model.similarity(data,'average')
  return x, y, z

# open input file: 
ifile = open('/home/tushar.abhishek/Musical_Instruments_5.json') 
all_data = list()
for i, line in enumerate(ifile): 
    # convert the json on this line to a dict
    data = json.loads(line)
    # extract what we want
    text = data['reviewText']
    prodid = data['asin']
    # add to the data collected so far
    all_data.append([prodid, text])
# create the DataFrame
df = pd.DataFrame(all_data, columns=['Product ID','Review'])
## Group by product ID
groupby_Id = df['Review'].groupby(df['Product ID'])
## z = No. of products
# z = len(list(groupby_Id))//100
z = 1
product_rating = [] ### Rating of each Product
index = 28
#### Classify each review index wise
for j in range(z):
  review = len(list(groupby_Id)[5][1])
  p = 0
  # print(t)
  for i in range(review):  ###No. of reviews of a product
    classify = [] 
    ### Text Preprocessing of reviews for each Product
    text,token_text = preprocess(list(groupby_Id)[5][1][index])
    # print('Pos Tag:',token_text)
    tag_list = []
    new_tokens = []
    for k in range(len(token_text)):
        tag_list.append(token_text[k][1])
    for k in range(len(tag_list)-1):
#### Adj noun combinations
       if ((tag_list[k] == 'JJ') and (tag_list[k+1] == 'NN' or tag_list[k] == 'NNP')):
            new_tokens.append(token_text[k][0])
            new_tokens.append(token_text[k+1][0])
#### noun/pronoun verb noun combinations
       if ((tag_list[k] == 'NN' or tag_list[k] == 'NNP') and (tag_list[k+1] == 'VB' or tag_list[k+1] == 'VBD' or  tag_list[k+1] == 'VBP')):
             new_tokens.append(token_text[k][0])
             new_tokens.append(token_text[k+1][0])
### noun/noun combinations
       if ((tag_list[k] == 'NN' or tag_list[k] == 'NNP') and (tag_list[k+1] == 'NN' or tag_list[k+1] == 'NNP')):
            new_tokens.append(token_text[k][0])
            new_tokens.append(token_text[k+1][0])
    if(np.isin('\n',new_tokens)):
      new_tokens.remove('\n')
    new_tokens = np.unique(new_tokens)
    print('text: ',text)
    print("New tokens: ",new_tokens)
    # print('\n')   ##### Rule Based Aspects
    #### x y z represent similarity with good bad average
    good_count = []
    bad_count = []
    average_count = []
    ###### if word not present in pretrained model
    for k in range(len(new_tokens)):
      #count = 0
      if (new_tokens[k] in word_vectors.vocab):
        x,y,z = word_embedding(new_tokens[k])
        good_count.append(x)
        bad_count.append(y)
        average_count.append(z)
    a = sum(good_count)
    b = sum(bad_count)
    c = sum(average_count)
    if a > b and a > c:
        classify.append('good')
        p +=5
    elif b > a and b > c:
        classify.append('bad')
        p +=1
    else :
        classify.append('average')
        p+=3
    index += 1
    rating = p / review
    print('rating:',rating)
    print(index , classify)
  # product_rating.append(rating)
# print('ratings',product_rating)

ifile.close()
